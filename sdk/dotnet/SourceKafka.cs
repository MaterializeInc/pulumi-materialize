// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Materialize
{
    /// <summary>
    /// A source describes an external system you want Materialize to read data from, and provides details about how to decode and interpret that data.
    /// 
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using Pulumi;
    /// using Materialize = Pulumi.Materialize;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var exampleSourceKafka = new Materialize.SourceKafka("exampleSourceKafka", new()
    ///     {
    ///         Envelope = new Materialize.Inputs.SourceKafkaEnvelopeArgs
    ///         {
    ///             None = true,
    ///         },
    ///         Format = new Materialize.Inputs.SourceKafkaFormatArgs
    ///         {
    ///             Avro = new Materialize.Inputs.SourceKafkaFormatAvroArgs
    ///             {
    ///                 SchemaRegistryConnection = new Materialize.Inputs.SourceKafkaFormatAvroSchemaRegistryConnectionArgs
    ///                 {
    ///                     DatabaseName = "database",
    ///                     Name = "csr_connection",
    ///                     SchemaName = "schema",
    ///                 },
    ///             },
    ///         },
    ///         KafkaConnection = new Materialize.Inputs.SourceKafkaKafkaConnectionArgs
    ///         {
    ///             DatabaseName = "database",
    ///             Name = "kafka_connection",
    ///             SchemaName = "schema",
    ///         },
    ///         SchemaName = "schema",
    ///         Size = "3xsmall",
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## Import
    /// 
    /// # Sources can be imported using the source id
    /// 
    /// ```sh
    ///  $ pulumi import materialize:index/sourceKafka:SourceKafka example_source_kafka &lt;source_id&gt;
    /// ```
    /// </summary>
    [MaterializeResourceType("materialize:index/sourceKafka:SourceKafka")]
    public partial class SourceKafka : global::Pulumi.CustomResource
    {
        /// <summary>
        /// The cluster to maintain this source. If not specified, the size option must be specified.
        /// </summary>
        [Output("clusterName")]
        public Output<string> ClusterName { get; private set; } = null!;

        /// <summary>
        /// The identifier for the source database.
        /// </summary>
        [Output("databaseName")]
        public Output<string?> DatabaseName { get; private set; } = null!;

        /// <summary>
        /// How Materialize should interpret records (e.g. append-only, upsert)..
        /// </summary>
        [Output("envelope")]
        public Output<Outputs.SourceKafkaEnvelope?> Envelope { get; private set; } = null!;

        /// <summary>
        /// How to decode raw bytes from different formats into data structures Materialize can understand at runtime.
        /// </summary>
        [Output("format")]
        public Output<Outputs.SourceKafkaFormat?> Format { get; private set; } = null!;

        /// <summary>
        /// Include message headers.
        /// </summary>
        [Output("includeHeaders")]
        public Output<bool?> IncludeHeaders { get; private set; } = null!;

        /// <summary>
        /// Include a column containing the Kafka message key. If the key is encoded using a format that includes schemas, the column will take its name from the schema. For unnamed formats (e.g. TEXT), the column will be named "key".
        /// </summary>
        [Output("includeKey")]
        public Output<bool?> IncludeKey { get; private set; } = null!;

        /// <summary>
        /// Include an offset column containing the Kafka message offset.
        /// </summary>
        [Output("includeOffset")]
        public Output<bool?> IncludeOffset { get; private set; } = null!;

        /// <summary>
        /// Include a partition column containing the Kafka message partition
        /// </summary>
        [Output("includePartition")]
        public Output<bool?> IncludePartition { get; private set; } = null!;

        /// <summary>
        /// Include a timestamp column containing the Kafka message timestamp.
        /// </summary>
        [Output("includeTimestamp")]
        public Output<bool?> IncludeTimestamp { get; private set; } = null!;

        /// <summary>
        /// The Kafka connection to use in the source.
        /// </summary>
        [Output("kafkaConnection")]
        public Output<Outputs.SourceKafkaKafkaConnection> KafkaConnection { get; private set; } = null!;

        /// <summary>
        /// Set the key format explicitly.
        /// </summary>
        [Output("keyFormat")]
        public Output<Outputs.SourceKafkaKeyFormat?> KeyFormat { get; private set; } = null!;

        /// <summary>
        /// The identifier for the source.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// Declare a set of columns as a primary key.
        /// </summary>
        [Output("primaryKeys")]
        public Output<ImmutableArray<string>> PrimaryKeys { get; private set; } = null!;

        /// <summary>
        /// The fully qualified name of the source.
        /// </summary>
        [Output("qualifiedSqlName")]
        public Output<string> QualifiedSqlName { get; private set; } = null!;

        /// <summary>
        /// The identifier for the source schema.
        /// </summary>
        [Output("schemaName")]
        public Output<string?> SchemaName { get; private set; } = null!;

        /// <summary>
        /// The size of the source.
        /// </summary>
        [Output("size")]
        public Output<string> Size { get; private set; } = null!;

        /// <summary>
        /// The type of source.
        /// </summary>
        [Output("sourceType")]
        public Output<string> SourceType { get; private set; } = null!;

        /// <summary>
        /// Read partitions from the specified offset.
        /// </summary>
        [Output("startOffsets")]
        public Output<ImmutableArray<int>> StartOffsets { get; private set; } = null!;

        /// <summary>
        /// Use the specified value to set "START OFFSET" based on the Kafka timestamp.
        /// </summary>
        [Output("startTimestamp")]
        public Output<int?> StartTimestamp { get; private set; } = null!;

        /// <summary>
        /// The Kafka topic you want to subscribe to.
        /// </summary>
        [Output("topic")]
        public Output<string> Topic { get; private set; } = null!;

        /// <summary>
        /// Set the value format explicitly.
        /// </summary>
        [Output("valueFormat")]
        public Output<Outputs.SourceKafkaValueFormat?> ValueFormat { get; private set; } = null!;


        /// <summary>
        /// Create a SourceKafka resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public SourceKafka(string name, SourceKafkaArgs args, CustomResourceOptions? options = null)
            : base("materialize:index/sourceKafka:SourceKafka", name, args ?? new SourceKafkaArgs(), MakeResourceOptions(options, ""))
        {
        }

        private SourceKafka(string name, Input<string> id, SourceKafkaState? state = null, CustomResourceOptions? options = null)
            : base("materialize:index/sourceKafka:SourceKafka", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
                PluginDownloadURL = "github://api.github.com/MaterializeInc/pulumi-materialize",
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing SourceKafka resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static SourceKafka Get(string name, Input<string> id, SourceKafkaState? state = null, CustomResourceOptions? options = null)
        {
            return new SourceKafka(name, id, state, options);
        }
    }

    public sealed class SourceKafkaArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The cluster to maintain this source. If not specified, the size option must be specified.
        /// </summary>
        [Input("clusterName")]
        public Input<string>? ClusterName { get; set; }

        /// <summary>
        /// The identifier for the source database.
        /// </summary>
        [Input("databaseName")]
        public Input<string>? DatabaseName { get; set; }

        /// <summary>
        /// How Materialize should interpret records (e.g. append-only, upsert)..
        /// </summary>
        [Input("envelope")]
        public Input<Inputs.SourceKafkaEnvelopeArgs>? Envelope { get; set; }

        /// <summary>
        /// How to decode raw bytes from different formats into data structures Materialize can understand at runtime.
        /// </summary>
        [Input("format")]
        public Input<Inputs.SourceKafkaFormatArgs>? Format { get; set; }

        /// <summary>
        /// Include message headers.
        /// </summary>
        [Input("includeHeaders")]
        public Input<bool>? IncludeHeaders { get; set; }

        /// <summary>
        /// Include a column containing the Kafka message key. If the key is encoded using a format that includes schemas, the column will take its name from the schema. For unnamed formats (e.g. TEXT), the column will be named "key".
        /// </summary>
        [Input("includeKey")]
        public Input<bool>? IncludeKey { get; set; }

        /// <summary>
        /// Include an offset column containing the Kafka message offset.
        /// </summary>
        [Input("includeOffset")]
        public Input<bool>? IncludeOffset { get; set; }

        /// <summary>
        /// Include a partition column containing the Kafka message partition
        /// </summary>
        [Input("includePartition")]
        public Input<bool>? IncludePartition { get; set; }

        /// <summary>
        /// Include a timestamp column containing the Kafka message timestamp.
        /// </summary>
        [Input("includeTimestamp")]
        public Input<bool>? IncludeTimestamp { get; set; }

        /// <summary>
        /// The Kafka connection to use in the source.
        /// </summary>
        [Input("kafkaConnection", required: true)]
        public Input<Inputs.SourceKafkaKafkaConnectionArgs> KafkaConnection { get; set; } = null!;

        /// <summary>
        /// Set the key format explicitly.
        /// </summary>
        [Input("keyFormat")]
        public Input<Inputs.SourceKafkaKeyFormatArgs>? KeyFormat { get; set; }

        /// <summary>
        /// The identifier for the source.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        [Input("primaryKeys")]
        private InputList<string>? _primaryKeys;

        /// <summary>
        /// Declare a set of columns as a primary key.
        /// </summary>
        public InputList<string> PrimaryKeys
        {
            get => _primaryKeys ?? (_primaryKeys = new InputList<string>());
            set => _primaryKeys = value;
        }

        /// <summary>
        /// The identifier for the source schema.
        /// </summary>
        [Input("schemaName")]
        public Input<string>? SchemaName { get; set; }

        /// <summary>
        /// The size of the source.
        /// </summary>
        [Input("size")]
        public Input<string>? Size { get; set; }

        [Input("startOffsets")]
        private InputList<int>? _startOffsets;

        /// <summary>
        /// Read partitions from the specified offset.
        /// </summary>
        public InputList<int> StartOffsets
        {
            get => _startOffsets ?? (_startOffsets = new InputList<int>());
            set => _startOffsets = value;
        }

        /// <summary>
        /// Use the specified value to set "START OFFSET" based on the Kafka timestamp.
        /// </summary>
        [Input("startTimestamp")]
        public Input<int>? StartTimestamp { get; set; }

        /// <summary>
        /// The Kafka topic you want to subscribe to.
        /// </summary>
        [Input("topic", required: true)]
        public Input<string> Topic { get; set; } = null!;

        /// <summary>
        /// Set the value format explicitly.
        /// </summary>
        [Input("valueFormat")]
        public Input<Inputs.SourceKafkaValueFormatArgs>? ValueFormat { get; set; }

        public SourceKafkaArgs()
        {
        }
        public static new SourceKafkaArgs Empty => new SourceKafkaArgs();
    }

    public sealed class SourceKafkaState : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The cluster to maintain this source. If not specified, the size option must be specified.
        /// </summary>
        [Input("clusterName")]
        public Input<string>? ClusterName { get; set; }

        /// <summary>
        /// The identifier for the source database.
        /// </summary>
        [Input("databaseName")]
        public Input<string>? DatabaseName { get; set; }

        /// <summary>
        /// How Materialize should interpret records (e.g. append-only, upsert)..
        /// </summary>
        [Input("envelope")]
        public Input<Inputs.SourceKafkaEnvelopeGetArgs>? Envelope { get; set; }

        /// <summary>
        /// How to decode raw bytes from different formats into data structures Materialize can understand at runtime.
        /// </summary>
        [Input("format")]
        public Input<Inputs.SourceKafkaFormatGetArgs>? Format { get; set; }

        /// <summary>
        /// Include message headers.
        /// </summary>
        [Input("includeHeaders")]
        public Input<bool>? IncludeHeaders { get; set; }

        /// <summary>
        /// Include a column containing the Kafka message key. If the key is encoded using a format that includes schemas, the column will take its name from the schema. For unnamed formats (e.g. TEXT), the column will be named "key".
        /// </summary>
        [Input("includeKey")]
        public Input<bool>? IncludeKey { get; set; }

        /// <summary>
        /// Include an offset column containing the Kafka message offset.
        /// </summary>
        [Input("includeOffset")]
        public Input<bool>? IncludeOffset { get; set; }

        /// <summary>
        /// Include a partition column containing the Kafka message partition
        /// </summary>
        [Input("includePartition")]
        public Input<bool>? IncludePartition { get; set; }

        /// <summary>
        /// Include a timestamp column containing the Kafka message timestamp.
        /// </summary>
        [Input("includeTimestamp")]
        public Input<bool>? IncludeTimestamp { get; set; }

        /// <summary>
        /// The Kafka connection to use in the source.
        /// </summary>
        [Input("kafkaConnection")]
        public Input<Inputs.SourceKafkaKafkaConnectionGetArgs>? KafkaConnection { get; set; }

        /// <summary>
        /// Set the key format explicitly.
        /// </summary>
        [Input("keyFormat")]
        public Input<Inputs.SourceKafkaKeyFormatGetArgs>? KeyFormat { get; set; }

        /// <summary>
        /// The identifier for the source.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        [Input("primaryKeys")]
        private InputList<string>? _primaryKeys;

        /// <summary>
        /// Declare a set of columns as a primary key.
        /// </summary>
        public InputList<string> PrimaryKeys
        {
            get => _primaryKeys ?? (_primaryKeys = new InputList<string>());
            set => _primaryKeys = value;
        }

        /// <summary>
        /// The fully qualified name of the source.
        /// </summary>
        [Input("qualifiedSqlName")]
        public Input<string>? QualifiedSqlName { get; set; }

        /// <summary>
        /// The identifier for the source schema.
        /// </summary>
        [Input("schemaName")]
        public Input<string>? SchemaName { get; set; }

        /// <summary>
        /// The size of the source.
        /// </summary>
        [Input("size")]
        public Input<string>? Size { get; set; }

        /// <summary>
        /// The type of source.
        /// </summary>
        [Input("sourceType")]
        public Input<string>? SourceType { get; set; }

        [Input("startOffsets")]
        private InputList<int>? _startOffsets;

        /// <summary>
        /// Read partitions from the specified offset.
        /// </summary>
        public InputList<int> StartOffsets
        {
            get => _startOffsets ?? (_startOffsets = new InputList<int>());
            set => _startOffsets = value;
        }

        /// <summary>
        /// Use the specified value to set "START OFFSET" based on the Kafka timestamp.
        /// </summary>
        [Input("startTimestamp")]
        public Input<int>? StartTimestamp { get; set; }

        /// <summary>
        /// The Kafka topic you want to subscribe to.
        /// </summary>
        [Input("topic")]
        public Input<string>? Topic { get; set; }

        /// <summary>
        /// Set the value format explicitly.
        /// </summary>
        [Input("valueFormat")]
        public Input<Inputs.SourceKafkaValueFormatGetArgs>? ValueFormat { get; set; }

        public SourceKafkaState()
        {
        }
        public static new SourceKafkaState Empty => new SourceKafkaState();
    }
}
